{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"trusted":true},"outputs":[],"source":["!pip install datasets==2.16.0 transformers lightning torchmetrics -q"]},{"cell_type":"code","execution_count":1,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"outputs":[],"source":["from datasets import load_dataset\n","\n","dataset = load_dataset(\"stanfordnlp/imdb\")"]},{"cell_type":"code","execution_count":2,"metadata":{"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}],"source":["from transformers import AutoTokenizer, AutoModelForSequenceClassification\n","\n","tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n","model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\")"]},{"cell_type":"code","execution_count":3,"metadata":{"trusted":true},"outputs":[{"data":{"text/plain":["DatasetDict({\n","    train: Dataset({\n","        features: ['text', 'label'],\n","        num_rows: 25000\n","    })\n","    test: Dataset({\n","        features: ['text', 'label'],\n","        num_rows: 25000\n","    })\n","    unsupervised: Dataset({\n","        features: ['text', 'label'],\n","        num_rows: 50000\n","    })\n","})"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["dataset"]},{"cell_type":"code","execution_count":4,"metadata":{"trusted":true},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import lightning as L\n","from torch.utils.data import DataLoader,Dataset\n","import torchmetrics\n","from lightning.pytorch.loggers import CSVLogger\n","import os\n","import time\n","from torch.utils.data.dataset import random_split"]},{"cell_type":"code","execution_count":5,"metadata":{"trusted":true},"outputs":[],"source":["def tokenize_ds(example):\n","    return tokenizer(example['text'],truncation=True,padding=True,max_length=tokenizer.model_max_length,return_tensors='pt')\n","ds = dataset.map(tokenize_ds,batched=True,batch_size=2000)"]},{"cell_type":"code","execution_count":6,"metadata":{"trusted":true},"outputs":[],"source":["ds.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["del dataset"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""]},{"cell_type":"code","execution_count":9,"metadata":{"trusted":true},"outputs":[],"source":["class IMDBDataset(Dataset):\n","    def __init__(self,partition,dataset=ds):\n","        super().__init__()\n","        self.partition = dataset[partition]\n","    def __len__(self):\n","        return len(self.partition)\n","    def __getitem__(self,idx):\n","        return self.partition[idx]"]},{"cell_type":"code","execution_count":10,"metadata":{"trusted":true},"outputs":[],"source":["class IMDB_Lightning(L.LightningDataModule):\n","    def __init__(self,batch_size=64):\n","        super().__init__()\n","        self.batch_size = batch_size\n","    def setup(self,stage: str):\n","        self.train_ds = IMDBDataset('train')\n","        self.val_ds,self.test_ds = random_split(IMDBDataset('test'),lengths=[20000,5000])\n","    def prepare_data(self):\n","        return\n","    def train_dataloader(self):\n","        return DataLoader(self.train_ds,self.batch_size,shuffle=True,drop_last=True,num_workers=10)\n","    def test_dataloader(self):\n","        return DataLoader(self.test_ds,self.batch_size,shuffle=False,num_workers=10)\n","    def val_dataloader(self):\n","        return DataLoader(self.val_ds,self.batch_size,shuffle=False)"]},{"cell_type":"code","execution_count":12,"metadata":{"trusted":true},"outputs":[],"source":["class LoRALayer(nn.Module):\n","    def __init__(self,inp_dim,out_dim,rank,alpha):\n","        super().__init__()\n","        sd = 1 / torch.tensor(rank,dtype=torch.float32) ** - 1/2\n","        self.A = torch.nn.Parameter(torch.randn(inp_dim,rank) * sd,requires_grad=True)\n","        self.B = torch.nn.Parameter(torch.zeros(rank,out_dim),requires_grad=True)\n","        self.alpha = alpha\n","    def forward(self,x):\n","        x = self.alpha * (x @ (self.A @ self.B))\n","        return x\n","    \n","class LoRALinearLayer(nn.Module):\n","    def __init__(self,linear,rank,alpha):\n","        super().__init__()\n","        self.linear = linear\n","        self.loralayer = LoRALayer(linear.in_features,linear.out_features,rank,alpha)\n","    def forward(self,x):\n","        x = self.linear(x) + self.loralayer(x)\n","        return x"]},{"cell_type":"code","execution_count":11,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["DistilBertForSequenceClassification(\n","  (distilbert): DistilBertModel(\n","    (embeddings): Embeddings(\n","      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n","      (position_embeddings): Embedding(512, 768)\n","      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (transformer): Transformer(\n","      (layer): ModuleList(\n","        (0-5): 6 x TransformerBlock(\n","          (attention): MultiHeadSelfAttention(\n","            (dropout): Dropout(p=0.1, inplace=False)\n","            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n","            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n","            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n","            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n","          )\n","          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (ffn): FFN(\n","            (dropout): Dropout(p=0.1, inplace=False)\n","            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n","            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n","            (activation): GELUActivation()\n","          )\n","          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        )\n","      )\n","    )\n","  )\n","  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n","  (classifier): Linear(in_features=768, out_features=2, bias=True)\n","  (dropout): Dropout(p=0.2, inplace=False)\n",")\n"]}],"source":["print(model)"]},{"cell_type":"code","execution_count":12,"metadata":{"trusted":true},"outputs":[],"source":["# for p in model.parameters():\n","#     p.requires_grad = False\n","    \n","# for param in model.pre_classifier.parameters():\n","#     param.requires_grad = True\n","\n","# for param in model.classifier.parameters():\n","#     param.requires_grad = True\n","\n","# for p in model.distilbert.transformer.layer:\n","#     p.attention.q_lin = LoRALinearLayer(linear=p.attention.q_lin,rank=4,alpha=1)\n","#     p.attention.v_lin = LoRALinearLayer(linear=p.attention.v_lin,rank=4,alpha=1)\n","\n","# model.classifier = LoRALinearLayer(linear=model.classifier,rank=4,alpha=1)"]},{"cell_type":"code","execution_count":13,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Finetuning the whole model\n","Total params : 66955010\n","Trainable params : 66955010\n"]}],"source":["print(\"Finetuning the whole model\")\n","print(\"Total params :\",sum([p.numel() for p in model.parameters()]))\n","print(\"Trainable params :\",sum([p.numel() for p in model.parameters() if p.requires_grad==True]))"]},{"cell_type":"code","execution_count":14,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["DistilBertForSequenceClassification(\n","  (distilbert): DistilBertModel(\n","    (embeddings): Embeddings(\n","      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n","      (position_embeddings): Embedding(512, 768)\n","      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (transformer): Transformer(\n","      (layer): ModuleList(\n","        (0-5): 6 x TransformerBlock(\n","          (attention): MultiHeadSelfAttention(\n","            (dropout): Dropout(p=0.1, inplace=False)\n","            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n","            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n","            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n","            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n","          )\n","          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (ffn): FFN(\n","            (dropout): Dropout(p=0.1, inplace=False)\n","            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n","            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n","            (activation): GELUActivation()\n","          )\n","          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        )\n","      )\n","    )\n","  )\n","  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n","  (classifier): Linear(in_features=768, out_features=2, bias=True)\n","  (dropout): Dropout(p=0.2, inplace=False)\n",")\n"]}],"source":["print(model)"]},{"cell_type":"code","execution_count":15,"metadata":{"trusted":true},"outputs":[],"source":["class LightningBERT(L.LightningModule):\n","    def __init__(self,model,lr):\n","        super().__init__()\n","        self.lr = lr\n","        self.model = model\n","        self.training_acc = torchmetrics.Accuracy(task='multiclass',num_classes=2)\n","        self.val_acc = torchmetrics.Accuracy(task='multiclass',num_classes=2)\n","        self.test_acc = torchmetrics.Accuracy(task='multiclass',num_classes=2)\n","    def forward(self,input_ids,attention_mask,labels):\n","        return self.model(input_ids=input_ids,attention_mask=attention_mask,labels=labels)\n","    def training_step(self,batch,batch_idx):\n","        outputs = self(input_ids=batch['input_ids'],attention_mask=batch['attention_mask'],labels=batch['label'])\n","        loss = outputs['loss']\n","        logits = outputs['logits']\n","        preds = logits.argmax(dim=1)\n","        self.training_acc(preds,batch['label'])\n","        self.log('training_loss',loss,prog_bar=True,on_step=False,on_epoch=True,sync_dist=True)\n","        self.log('training_acc',self.training_acc,prog_bar=True,on_step=False,on_epoch=True,sync_dist=True)\n","        return loss\n","    def validation_step(self,batch,batch_idx):\n","        outputs = self(input_ids=batch['input_ids'],attention_mask=batch['attention_mask'],labels=batch['label'])\n","        loss = outputs['loss']\n","        logits = outputs['logits']\n","        preds = logits.argmax(dim=1)\n","        self.val_acc(preds,batch['label'])\n","        self.log('validation_loss',loss,prog_bar=True,on_step=False,on_epoch=True,sync_dist=True)\n","        self.log('validation_acc',self.val_acc,prog_bar=True,on_step=False,on_epoch=True,sync_dist=True)\n","    def test_step(self,batch,batch_idx):\n","        outputs = self(input_ids=batch['input_ids'],attention_mask=batch['attention_mask'],labels=batch['label'])\n","        loss = outputs['loss']\n","        logits = outputs['logits']\n","        preds = logits.argmax(dim=1)\n","        self.test_acc(preds,batch['label'])\n","        self.log('test_acc',self.test_acc)\n","    def configure_optimizers(self):\n","        opt = torch.optim.AdamW(self.parameters(),lr=self.lr)\n","        return opt"]},{"cell_type":"code","execution_count":16,"metadata":{"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Using 16bit Automatic Mixed Precision (AMP)\n","GPU available: True (cuda), used: True\n","TPU available: False, using: 0 TPU cores\n","IPU available: False, using: 0 IPUs\n","HPU available: False, using: 0 HPUs\n"]},{"name":"stderr","output_type":"stream","text":["You are using a CUDA device ('NVIDIA A10G') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","\n","  | Name         | Type                                | Params\n","---------------------------------------------------------------------\n","0 | model        | DistilBertForSequenceClassification | 67.0 M\n","1 | training_acc | MulticlassAccuracy                  | 0     \n","2 | val_acc      | MulticlassAccuracy                  | 0     \n","3 | test_acc     | MulticlassAccuracy                  | 0     \n","---------------------------------------------------------------------\n","67.0 M    Trainable params\n","0         Non-trainable params\n","67.0 M    Total params\n","267.820   Total estimated model params size (MB)\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f34fe87a814b4ce6b1e96134f90a6512","version_major":2,"version_minor":0},"text/plain":["Sanity Checking: |          | 0/? [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"69d25c4b743e4b5fbf7991330d6a25ca","version_major":2,"version_minor":0},"text/plain":["Training: |          | 0/? [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d0a72e680983476587a3bfb7371ee106","version_major":2,"version_minor":0},"text/plain":["Validation: |          | 0/? [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"03fa073777344a12bde6bb86e17368f1","version_major":2,"version_minor":0},"text/plain":["Validation: |          | 0/? [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"9f529eb2e0b14039bfd9656690019b17","version_major":2,"version_minor":0},"text/plain":["Validation: |          | 0/? [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["`Trainer.fit` stopped: `max_epochs=3` reached.\n","/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/checkpoint_connector.py:145: `.test(ckpt_path=None)` was called without a model. The best model of the previous `fit` call will be used. You can pass `.test(ckpt_path='best')` to use the best model or `.test(ckpt_path='last')` to use the last model. If you pass a value, this warning will be silenced.\n","Restoring states from the checkpoint path at logs/my-model/version_28/checkpoints/epoch=1-step=780.ckpt\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","Loaded model weights from the checkpoint at logs/my-model/version_28/checkpoints/epoch=1-step=780.ckpt\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"eb4154fd20fa4cf5a62682c5407dc339","version_major":2,"version_minor":0},"text/plain":["Testing: |          | 0/? [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n","┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n","│<span style=\"color: #008080; text-decoration-color: #008080\">         test_acc          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.9287999868392944     </span>│\n","└───────────────────────────┴───────────────────────────┘\n","</pre>\n"],"text/plain":["┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n","┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n","│\u001b[36m \u001b[0m\u001b[36m        test_acc         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.9287999868392944    \u001b[0m\u001b[35m \u001b[0m│\n","└───────────────────────────┴───────────────────────────┘\n"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Finetuning the whole model\n","Total params : 66955010\n","Trainable params : 66955010\n","Training Time : 10.92\n"]}],"source":["numepochs = 3\n","# Instantiate the Lightning DataModule\n","dm = IMDB_Lightning(batch_size=64)\n","dm.setup(stage='fit')\n","total_steps = len(dm.train_dataloader()) * numepochs\n","logger = CSVLogger(save_dir=\"logs/\", name=\"my-model\")\n","# instantiate the models and define the trainer and begin training\n","\n","lightning_bert = LightningBERT(model=model,lr=5e-5)\n","callback = L.pytorch.callbacks.ModelCheckpoint(save_top_k=1,mode='max',monitor='validation_acc',save_last=True)\n","trainer = L.Trainer(\n","    accelerator='gpu',\n","    devices=1,\n","    max_epochs=numepochs,\n","    callbacks=[callback],\n","    logger=logger,\n","    precision='16-mixed'\n",")\n","s = time.time()\n","trainer.fit(\n","    model=lightning_bert,\n","    datamodule=dm,\n",")\n","e = time.time()\n","test_acc = trainer.test(dataloaders=dm.test_dataloader())[0]['test_acc']\n","print(\"Finetuning the whole model\")\n","print(\"Total params :\",sum([p.numel() for p in model.parameters()]))\n","print(\"Trainable params :\",sum([p.numel() for p in model.parameters() if p.requires_grad==True]))\n","print(f\"Training Time : {(e - s)/60 :.2f}\")"]},{"cell_type":"code","execution_count":21,"metadata":{"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/checkpoint_connector.py:145: `.test(ckpt_path=None)` was called without a model. The best model of the previous `fit` call will be used. You can pass `.test(ckpt_path='best')` to use the best model or `.test(ckpt_path='last')` to use the last model. If you pass a value, this warning will be silenced.\n"]},{"ename":"MisconfigurationException","evalue":"No `test_step()` method defined to run `Trainer.test`.","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mMisconfigurationException\u001b[0m                 Traceback (most recent call last)","\u001b[1;32m/teamspace/studios/this_studio/last-two-layers.ipynb Cell 19\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://vscode-01hv6m8fh6jmycjm6pj0548wbm.studio.lightning.ai/teamspace/studios/this_studio/last-two-layers.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m test_acc \u001b[39m=\u001b[39m trainer\u001b[39m.\u001b[39;49mtest(dataloaders\u001b[39m=\u001b[39;49mdm\u001b[39m.\u001b[39;49mtest_dataloader())[\u001b[39m0\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mtest_acc\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m      <a href='vscode-notebook-cell://vscode-01hv6m8fh6jmycjm6pj0548wbm.studio.lightning.ai/teamspace/studios/this_studio/last-two-layers.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mTraining the last two layers\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell://vscode-01hv6m8fh6jmycjm6pj0548wbm.studio.lightning.ai/teamspace/studios/this_studio/last-two-layers.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mTotal params :\u001b[39m\u001b[39m\"\u001b[39m,\u001b[39msum\u001b[39m([p\u001b[39m.\u001b[39mnumel() \u001b[39mfor\u001b[39;00m p \u001b[39min\u001b[39;00m model\u001b[39m.\u001b[39mparameters()]))\n","File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py:754\u001b[0m, in \u001b[0;36mTrainer.test\u001b[0;34m(self, model, dataloaders, ckpt_path, verbose, datamodule)\u001b[0m\n\u001b[1;32m    752\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mstatus \u001b[39m=\u001b[39m TrainerStatus\u001b[39m.\u001b[39mRUNNING\n\u001b[1;32m    753\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtesting \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m--> 754\u001b[0m \u001b[39mreturn\u001b[39;00m call\u001b[39m.\u001b[39;49m_call_and_handle_interrupt(\n\u001b[1;32m    755\u001b[0m     \u001b[39mself\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_test_impl, model, dataloaders, ckpt_path, verbose, datamodule\n\u001b[1;32m    756\u001b[0m )\n","File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py:44\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[39mif\u001b[39;00m trainer\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39mlauncher \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     43\u001b[0m         \u001b[39mreturn\u001b[39;00m trainer\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39mlauncher\u001b[39m.\u001b[39mlaunch(trainer_fn, \u001b[39m*\u001b[39margs, trainer\u001b[39m=\u001b[39mtrainer, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m---> 44\u001b[0m     \u001b[39mreturn\u001b[39;00m trainer_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     46\u001b[0m \u001b[39mexcept\u001b[39;00m _TunerExitException:\n\u001b[1;32m     47\u001b[0m     _call_teardown_hook(trainer)\n","File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py:794\u001b[0m, in \u001b[0;36mTrainer._test_impl\u001b[0;34m(self, model, dataloaders, ckpt_path, verbose, datamodule)\u001b[0m\n\u001b[1;32m    790\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mfn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    791\u001b[0m ckpt_path \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_checkpoint_connector\u001b[39m.\u001b[39m_select_ckpt_path(\n\u001b[1;32m    792\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mfn, ckpt_path, model_provided\u001b[39m=\u001b[39mmodel_provided, model_connected\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlightning_module \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    793\u001b[0m )\n\u001b[0;32m--> 794\u001b[0m results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run(model, ckpt_path\u001b[39m=\u001b[39;49mckpt_path)\n\u001b[1;32m    795\u001b[0m \u001b[39m# remove the tensors from the test results\u001b[39;00m\n\u001b[1;32m    796\u001b[0m results \u001b[39m=\u001b[39m convert_tensors_to_scalars(results)\n","File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py:937\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m    934\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_callback_connector\u001b[39m.\u001b[39m_attach_model_callbacks()\n\u001b[1;32m    935\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_callback_connector\u001b[39m.\u001b[39m_attach_model_logging_functions()\n\u001b[0;32m--> 937\u001b[0m _verify_loop_configurations(\u001b[39mself\u001b[39;49m)\n\u001b[1;32m    939\u001b[0m \u001b[39m# ----------------------------\u001b[39;00m\n\u001b[1;32m    940\u001b[0m \u001b[39m# SET UP THE TRAINER\u001b[39;00m\n\u001b[1;32m    941\u001b[0m \u001b[39m# ----------------------------\u001b[39;00m\n\u001b[1;32m    942\u001b[0m log\u001b[39m.\u001b[39mdebug(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m: setting up strategy environment\u001b[39m\u001b[39m\"\u001b[39m)\n","File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/lightning/pytorch/trainer/configuration_validator.py:42\u001b[0m, in \u001b[0;36m_verify_loop_configurations\u001b[0;34m(trainer)\u001b[0m\n\u001b[1;32m     40\u001b[0m     __verify_eval_loop_configuration(model, \u001b[39m\"\u001b[39m\u001b[39mval\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     41\u001b[0m \u001b[39melif\u001b[39;00m trainer\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mfn \u001b[39m==\u001b[39m TrainerFn\u001b[39m.\u001b[39mTESTING:\n\u001b[0;32m---> 42\u001b[0m     __verify_eval_loop_configuration(model, \u001b[39m\"\u001b[39;49m\u001b[39mtest\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m     43\u001b[0m \u001b[39melif\u001b[39;00m trainer\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mfn \u001b[39m==\u001b[39m TrainerFn\u001b[39m.\u001b[39mPREDICTING:\n\u001b[1;32m     44\u001b[0m     __verify_eval_loop_configuration(model, \u001b[39m\"\u001b[39m\u001b[39mpredict\u001b[39m\u001b[39m\"\u001b[39m)\n","File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/lightning/pytorch/trainer/configuration_validator.py:110\u001b[0m, in \u001b[0;36m__verify_eval_loop_configuration\u001b[0;34m(model, stage)\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m has_step:\n\u001b[1;32m    109\u001b[0m     trainer_method \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mvalidate\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m stage \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mval\u001b[39m\u001b[39m\"\u001b[39m \u001b[39melse\u001b[39;00m stage\n\u001b[0;32m--> 110\u001b[0m     \u001b[39mraise\u001b[39;00m MisconfigurationException(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mNo `\u001b[39m\u001b[39m{\u001b[39;00mstep_name\u001b[39m}\u001b[39;00m\u001b[39m()` method defined to run `Trainer.\u001b[39m\u001b[39m{\u001b[39;00mtrainer_method\u001b[39m}\u001b[39;00m\u001b[39m`.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    112\u001b[0m \u001b[39m# check legacy hooks are not present\u001b[39;00m\n\u001b[1;32m    113\u001b[0m epoch_end_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mvalidation_epoch_end\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m stage \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mval\u001b[39m\u001b[39m\"\u001b[39m \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mtest_epoch_end\u001b[39m\u001b[39m\"\u001b[39m\n","\u001b[0;31mMisconfigurationException\u001b[0m: No `test_step()` method defined to run `Trainer.test`."]}],"source":["test_acc = trainer.test(dataloaders=dm.test_dataloader())[0]['test_acc']\n","print(\"Training the last two layers\")\n","print(\"Total params :\",sum([p.numel() for p in model.parameters()]))\n","print(\"Trainable params :\",sum([p.numel() for p in model.parameters() if p.requires_grad==True]))\n","print(f\"Training Time : {(e - s)/60 :.2f}\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/checkpoint_connector.py:145: `.validate(ckpt_path=None)` was called without a model. The best model of the previous `fit` call will be used. You can pass `.validate(ckpt_path='best')` to use the best model or `.validate(ckpt_path='last')` to use the last model. If you pass a value, this warning will be silenced.\n","Restoring states from the checkpoint path at logs/my-model/version_7/checkpoints/epoch=2-step=1170.ckpt\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","Loaded model weights from the checkpoint at logs/my-model/version_7/checkpoints/epoch=2-step=1170.ckpt\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"46768c87918c4cd69eb7062f51f1ad59","version_major":2,"version_minor":0},"text/plain":["Validation: |          | 0/? [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n","┃<span style=\"font-weight: bold\">      Validate metric      </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n","│<span style=\"color: #008080; text-decoration-color: #008080\">      validation_acc       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.9061999917030334     </span>│\n","│<span style=\"color: #008080; text-decoration-color: #008080\">      validation_loss      </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.2390432357788086     </span>│\n","└───────────────────────────┴───────────────────────────┘\n","</pre>\n"],"text/plain":["┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n","┃\u001b[1m \u001b[0m\u001b[1m     Validate metric     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n","│\u001b[36m \u001b[0m\u001b[36m     validation_acc      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.9061999917030334    \u001b[0m\u001b[35m \u001b[0m│\n","│\u001b[36m \u001b[0m\u001b[36m     validation_loss     \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.2390432357788086    \u001b[0m\u001b[35m \u001b[0m│\n","└───────────────────────────┴───────────────────────────┘\n"]},"metadata":{},"output_type":"display_data"},{"ename":"KeyError","evalue":"'test_acc'","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","\u001b[1;32m/teamspace/studios/this_studio/lora-from-scratch.ipynb Cell 20\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://vscode-01hv6m8fh6jmycjm6pj0548wbm.studio.lightning.ai/teamspace/studios/this_studio/lora-from-scratch.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m test_acc \u001b[39m=\u001b[39m trainer\u001b[39m.\u001b[39;49mvalidate(dataloaders\u001b[39m=\u001b[39;49mdm\u001b[39m.\u001b[39;49mtest_dataloader())[\u001b[39m0\u001b[39;49m][\u001b[39m'\u001b[39;49m\u001b[39mtest_acc\u001b[39;49m\u001b[39m'\u001b[39;49m]\n","\u001b[0;31mKeyError\u001b[0m: 'test_acc'"]}],"source":["test_acc = trainer.test(dataloaders=dm.test_dataloader())[0]['test_acc']"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[{"ename":"NameError","evalue":"name 'torch' is not defined","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[1;32m/teamspace/studios/this_studio/normal-finetuning.ipynb Cell 21\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://vscode-01hv6m8fh6jmycjm6pj0548wbm.studio.lightning.ai/teamspace/studios/this_studio/normal-finetuning.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mempty_cache()\n","\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"]}],"source":["torch.cuda.empty_cache()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30683,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.10"}},"nbformat":4,"nbformat_minor":4}
