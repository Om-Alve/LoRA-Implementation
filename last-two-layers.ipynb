{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install datasets==2.16.0 transformers lightning torchmetrics -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"stanfordnlp/imdb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    unsupervised: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 50000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import lightning as L\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "import torchmetrics\n",
    "from lightning.pytorch.loggers import CSVLogger\n",
    "import os\n",
    "import time\n",
    "from torch.utils.data.dataset import random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_ds(example):\n",
    "    return tokenizer(example['text'],truncation=True,padding=True,max_length=tokenizer.model_max_length,return_tensors='pt')\n",
    "ds = dataset.map(tokenize_ds,batched=True,batch_size=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "del dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IMDBDataset(Dataset):\n",
    "    def __init__(self,partition,dataset=ds):\n",
    "        super().__init__()\n",
    "        self.partition = dataset[partition]\n",
    "    def __len__(self):\n",
    "        return len(self.partition)\n",
    "    def __getitem__(self,idx):\n",
    "        return self.partition[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IMDB_Lightning(L.LightningDataModule):\n",
    "    def __init__(self,batch_size=64):\n",
    "        super().__init__()\n",
    "        self.batch_size = batch_size\n",
    "    def setup(self,stage: str):\n",
    "        self.train_ds = IMDBDataset('train')\n",
    "        self.val_ds,self.test_ds = random_split(IMDBDataset('test'),lengths=[20000,5000])\n",
    "    def prepare_data(self):\n",
    "        return\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_ds,self.batch_size,shuffle=True,drop_last=True,num_workers=10)\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.test_ds,self.batch_size,shuffle=False,num_workers=10)\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_ds,self.batch_size,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoRALayer(nn.Module):\n",
    "    def __init__(self,inp_dim,out_dim,rank,alpha):\n",
    "        super().__init__()\n",
    "        sd = 1 / torch.tensor(rank,dtype=torch.float32) ** - 1/2\n",
    "        self.A = torch.nn.Parameter(torch.randn(inp_dim,rank) * sd,requires_grad=True)\n",
    "        self.B = torch.nn.Parameter(torch.zeros(rank,out_dim),requires_grad=True)\n",
    "        self.alpha = alpha\n",
    "    def forward(self,x):\n",
    "        x = self.alpha * (x @ (self.A @ self.B))\n",
    "        return x\n",
    "    \n",
    "class LoRALinearLayer(nn.Module):\n",
    "    def __init__(self,linear,rank,alpha):\n",
    "        super().__init__()\n",
    "        self.linear = linear\n",
    "        self.loralayer = LoRALayer(linear.in_features,linear.out_features,rank,alpha)\n",
    "    def forward(self,x):\n",
    "        x = self.linear(x) + self.loralayer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DistilBertForSequenceClassification(\n",
      "  (distilbert): DistilBertModel(\n",
      "    (embeddings): Embeddings(\n",
      "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (transformer): Transformer(\n",
      "      (layer): ModuleList(\n",
      "        (0-5): 6 x TransformerBlock(\n",
      "          (attention): MultiHeadSelfAttention(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (ffn): FFN(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (activation): GELUActivation()\n",
      "          )\n",
      "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in model.parameters():\n",
    "    p.requires_grad = False\n",
    "    \n",
    "for param in model.pre_classifier.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "for param in model.classifier.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# for p in model.distilbert.transformer.layer:\n",
    "#     p.attention.q_lin = LoRALinearLayer(linear=p.attention.q_lin,rank=4,alpha=1)\n",
    "#     p.attention.v_lin = LoRALinearLayer(linear=p.attention.v_lin,rank=4,alpha=1)\n",
    "\n",
    "# model.classifier = LoRALinearLayer(linear=model.classifier,rank=4,alpha=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the last two layers\n",
      "Total params : 66955010\n",
      "Trainable params : 592130\n"
     ]
    }
   ],
   "source": [
    "print(\"Training the last two layers\")\n",
    "print(\"Total params :\",sum([p.numel() for p in model.parameters()]))\n",
    "print(\"Trainable params :\",sum([p.numel() for p in model.parameters() if p.requires_grad==True]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DistilBertForSequenceClassification(\n",
      "  (distilbert): DistilBertModel(\n",
      "    (embeddings): Embeddings(\n",
      "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (transformer): Transformer(\n",
      "      (layer): ModuleList(\n",
      "        (0-5): 6 x TransformerBlock(\n",
      "          (attention): MultiHeadSelfAttention(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (ffn): FFN(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (activation): GELUActivation()\n",
      "          )\n",
      "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LightningBERT(L.LightningModule):\n",
    "    def __init__(self,model,lr):\n",
    "        super().__init__()\n",
    "        self.lr = lr\n",
    "        self.model = model\n",
    "        self.training_acc = torchmetrics.Accuracy(task='multiclass',num_classes=2)\n",
    "        self.val_acc = torchmetrics.Accuracy(task='multiclass',num_classes=2)\n",
    "        self.test_acc = torchmetrics.Accuracy(task='multiclass',num_classes=2)\n",
    "    def forward(self,input_ids,attention_mask,labels):\n",
    "        return self.model(input_ids=input_ids,attention_mask=attention_mask,labels=labels)\n",
    "    def training_step(self,batch,batch_idx):\n",
    "        outputs = self(input_ids=batch['input_ids'],attention_mask=batch['attention_mask'],labels=batch['label'])\n",
    "        loss = outputs['loss']\n",
    "        logits = outputs['logits']\n",
    "        preds = logits.argmax(dim=1)\n",
    "        self.training_acc(preds,batch['label'])\n",
    "        self.log('training_loss',loss,prog_bar=True,on_step=False,on_epoch=True,sync_dist=True)\n",
    "        self.log('training_acc',self.training_acc,prog_bar=True,on_step=False,on_epoch=True,sync_dist=True)\n",
    "        return loss\n",
    "    def validation_step(self,batch,batch_idx):\n",
    "        outputs = self(input_ids=batch['input_ids'],attention_mask=batch['attention_mask'],labels=batch['label'])\n",
    "        loss = outputs['loss']\n",
    "        logits = outputs['logits']\n",
    "        preds = logits.argmax(dim=1)\n",
    "        self.val_acc(preds,batch['label'])\n",
    "        self.log('validation_loss',loss,prog_bar=True,on_step=False,on_epoch=True,sync_dist=True)\n",
    "        self.log('validation_acc',self.val_acc,prog_bar=True,on_step=False,on_epoch=True,sync_dist=True)\n",
    "    def test_step(self,batch,batch_idx):\n",
    "        outputs = self(input_ids=batch['input_ids'],attention_mask=batch['attention_mask'],labels=batch['label'])\n",
    "        loss = outputs['loss']\n",
    "        logits = outputs['logits']\n",
    "        preds = logits.argmax(dim=1)\n",
    "        self.test_acc(preds,batch['label'])\n",
    "        self.log('test_acc',self.test_acc)\n",
    "    def configure_optimizers(self):\n",
    "        opt = torch.optim.AdamW(self.parameters(),lr=self.lr)\n",
    "        return opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name         | Type                                | Params\n",
      "---------------------------------------------------------------------\n",
      "0 | model        | DistilBertForSequenceClassification | 67.0 M\n",
      "1 | training_acc | MulticlassAccuracy                  | 0     \n",
      "2 | val_acc      | MulticlassAccuracy                  | 0     \n",
      "3 | test_acc     | MulticlassAccuracy                  | 0     \n",
      "---------------------------------------------------------------------\n",
      "592 K     Trainable params\n",
      "66.4 M    Non-trainable params\n",
      "67.0 M    Total params\n",
      "267.820   Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34266da7657d45ec96dfcdfd2aa4f8a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca0c835e43534483b14334f97e6cb3aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ac36d949c2f47d7a7798e7386850121",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "885b9d9139794b738670c081232d914c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28e95aae735843d694dde69dccc17dcd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=3` reached.\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/checkpoint_connector.py:145: `.test(ckpt_path=None)` was called without a model. The best model of the previous `fit` call will be used. You can pass `.test(ckpt_path='best')` to use the best model or `.test(ckpt_path='last')` to use the last model. If you pass a value, this warning will be silenced.\n",
      "Restoring states from the checkpoint path at logs/my-model/version_10/checkpoints/epoch=1-step=780.ckpt\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Loaded model weights from the checkpoint at logs/my-model/version_10/checkpoints/epoch=1-step=780.ckpt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7de3f34c4e947d48cabffdbb54284ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_acc          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.8705999851226807     </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test_acc         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.8705999851226807    \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the last two layers\n",
      "Total params : 66955010\n",
      "Trainable params : 592130\n",
      "Training Time : 5.41\n"
     ]
    }
   ],
   "source": [
    "numepochs = 3\n",
    "# Instantiate the Lightning DataModule\n",
    "dm = IMDB_Lightning(batch_size=64)\n",
    "dm.setup(stage='fit')\n",
    "total_steps = len(dm.train_dataloader()) * numepochs\n",
    "logger = CSVLogger(save_dir=\"logs/\", name=\"my-model\")\n",
    "# instantiate the models and define the trainer and begin training\n",
    "\n",
    "lightning_bert = LightningBERT(model=model,lr=3e-4)\n",
    "callback = L.pytorch.callbacks.ModelCheckpoint(save_top_k=1,mode='max',monitor='validation_acc',save_last=True)\n",
    "trainer = L.Trainer(\n",
    "    accelerator='gpu',\n",
    "    devices=1,\n",
    "    max_epochs=numepochs,\n",
    "    callbacks=[callback],\n",
    "    logger=logger,\n",
    "    precision='16-mixed'\n",
    ")\n",
    "s = time.time()\n",
    "trainer.fit(\n",
    "    model=lightning_bert,\n",
    "    datamodule=dm,\n",
    ")\n",
    "e = time.time()\n",
    "test_acc = trainer.test(dataloaders=dm.test_dataloader())[0]['test_acc']\n",
    "print(\"Training the last two layers\")\n",
    "print(\"Total params :\",sum([p.numel() for p in model.parameters()]))\n",
    "print(\"Trainable params :\",sum([p.numel() for p in model.parameters() if p.requires_grad==True]))\n",
    "print(f\"Training Time : {(e - s)/60 :.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 30683,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
